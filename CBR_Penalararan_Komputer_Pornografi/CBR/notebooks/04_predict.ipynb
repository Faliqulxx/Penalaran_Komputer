{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8IMTPDQ3aEKT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZJsv7DjYaGkM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\\notebooks\\venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OA2MJ37daHyq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (0.33.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: filelock in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (3.12.13)\n",
      "Requirement already satisfied: packaging in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (2024.9.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (6.5.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "0XW_qgLUaTtL"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 02:46:37,050 - INFO - Directory ensured: c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\logs\n",
      "2025-06-26 02:46:37,052 - INFO - Directory ensured: c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\processed\n",
      "2025-06-26 02:46:37,054 - INFO - Directory ensured: c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\eval\n",
      "2025-06-26 02:46:37,055 - INFO - Directory ensured: c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\results\n",
      "2025-06-26 02:46:37,057 - INFO - Starting prediction process at 2025-06-26 02:46:37\n",
      "2025-06-26 02:46:37,069 - INFO - Loaded 50 cases from c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\processed\\cases.csv and 5 queries from c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\eval\\queries.json\n",
      "2025-06-26 02:46:37,072 - INFO - TF-IDF matrix shape: (50, 138)\n",
      "2025-06-26 02:46:37,092 - INFO - Training data shape: (50, 279)\n",
      "2025-06-26 02:46:37,093 - INFO - Class distribution: [25 25]\n",
      "2025-06-26 02:46:40,926 - INFO - Best Logistic Regression Parameters: {'C': 0.1}\n",
      "2025-06-26 02:46:40,927 - INFO - Best CV F1 Score (LogReg): 1.00\n",
      "2025-06-26 02:46:40,956 - INFO - Best SVM Parameters: {'C': 0.1}\n",
      "2025-06-26 02:46:40,956 - INFO - Best CV F1 Score (SVM): 1.00\n",
      "2025-06-26 02:46:40,957 - INFO - Loading IndoBERT model on cpu\n",
      "2025-06-26 02:46:40,961 - INFO - Load pretrained SentenceTransformer: indobenchmark/indobert-base-p1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Logistic Regression Parameters: {'C': 0.1}\n",
      "Best CV F1 Score (LogReg): 1.00\n",
      "Best SVM Parameters: {'C': 0.1}\n",
      "Best CV F1 Score (SVM): 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 02:46:41,781 - WARNING - No sentence-transformers model found with name indobenchmark/indobert-base-p1. Creating a new one with mean pooling.\n",
      "2025-06-26 02:46:46,080 - INFO - Document embeddings shape: (50, 768)\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.74it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 17.85it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.86it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.07it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.31it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.96it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]t/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.76it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.02it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.77it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.77it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.77it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.04it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.29it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.02it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.65it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 19.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.39it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.83it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 34.48it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 32.26it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.32it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.73it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.71it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.28it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.29it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.25it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 23.26it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.03it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 21.96it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 24.99it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.02it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.57it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.82it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.33it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.13it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 29.41it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.78it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.96it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.02it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.25it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 26.31it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.70it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.39it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.14it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 15.62it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 28.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.64it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 30.30it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 25.00it/s]it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.37it/s]\n",
      "Predicting: 100%|██████████| 50/50 [00:07<00:00,  6.28it/s]\n",
      "2025-06-26 02:46:54,044 - INFO - Saved results to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\results/logreg_predictions.csv\n",
      "2025-06-26 02:46:54,045 - INFO - Saved results to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\results/svm_predictions.csv\n",
      "2025-06-26 02:46:54,046 - INFO - Saved results to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\results/indobert_predictions.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\results/logreg_predictions.csv\n",
      "✅ Saved to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\results/svm_predictions.csv\n",
      "✅ Saved to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\data\\results/indobert_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Import Libraries and Initialize\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from typing import List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "\n",
    "# Define base directory (aligned with previous scripts)\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.getcwd())  # Parent of 'notebooks'\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # Fallback for interactive environments like Jupyter\n",
    "\n",
    "# Define paths\n",
    "PATH_CSV = os.path.join(BASE_DIR, 'data', 'processed', 'cases.csv')\n",
    "PATH_QUERIES = os.path.join(BASE_DIR, 'data', 'eval', 'queries.json')\n",
    "PATH_RESULTS = os.path.join(BASE_DIR, 'data', 'results')\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "LOG_PATH = os.path.join(LOG_DIR, 'prediction.log')\n",
    "\n",
    "# Validate path length for Windows\n",
    "MAX_PATH_LENGTH = 260\n",
    "\n",
    "def validate_path(path):\n",
    "    if len(path) > MAX_PATH_LENGTH:\n",
    "        raise ValueError(f\"Path {path} exceeds Windows maximum length of {MAX_PATH_LENGTH} characters\")\n",
    "    return path\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [LOG_DIR, os.path.dirname(PATH_CSV), os.path.dirname(PATH_QUERIES), PATH_RESULTS]:\n",
    "    try:\n",
    "        validate_path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logging.info(f\"Directory ensured: {path}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Path validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH, mode='a', encoding='utf-8'),  # Append mode\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.info(f\"Starting prediction process at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Part 2: Setup Environment\n",
    "def setup_environment():\n",
    "    \"\"\"Ensure NLTK resources are available.\"\"\"\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        logger.info(\"Downloading NLTK punkt tokenizer\")\n",
    "        nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Part 3: Preprocess Text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Preprocess text: lowercase, remove specific legal terms, tokenize, normalize spaces.\"\"\"\n",
    "    try:\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'\\b(?:putusan|nomor_perkara|tahun|pengadilan|hakim)\\b', '', text)\n",
    "        text = re.sub(r'uu\\s+no', 'undang_undang_nomor', text)\n",
    "        text = re.sub(r'pasal\\s+\\d+', 'pasal', text)\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        tokens = word_tokenize(text)\n",
    "        return ' '.join(tokens) if tokens else 'empty'\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing text: {e}\")\n",
    "        return 'empty'\n",
    "\n",
    "# Part 4: Load Data\n",
    "def load_data():\n",
    "    \"\"\"Load cases.csv and queries.json.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(PATH_CSV)\n",
    "        texts = df['ringkasan_fakta'].fillna('').apply(preprocess_text).tolist()\n",
    "        case_ids = df['case_id'].tolist()\n",
    "        if not texts or not case_ids:\n",
    "            raise ValueError(\"Dataset is empty or missing required columns ('ringkasan_fakta', 'case_id').\")\n",
    "        with open(PATH_QUERIES, 'r', encoding='utf-8') as f:\n",
    "            queries = json.load(f)\n",
    "        case_solutions = {item['case_id']: item.get('solution', '') for item in queries}\n",
    "        logger.info(f\"Loaded {len(df)} cases from {PATH_CSV} and {len(queries)} queries from {PATH_QUERIES}\")\n",
    "        return df, texts, case_ids, queries, case_solutions\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        print(f\"Error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading data: {e}\")\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 5: Setup TF-IDF\n",
    "def setup_tfidf(texts: List[str]) -> tuple:\n",
    "    \"\"\"Initialize TF-IDF vectorizer with reduced features.\"\"\"\n",
    "    stop_words = [\n",
    "        'dan', 'di', 'dari', 'ke', 'pada', 'dengan', 'untuk', 'yang', 'ini', 'itu',\n",
    "        'adalah', 'tersebut', 'sebagai', 'oleh', 'atau', 'tetapi', 'karena', 'jika',\n",
    "        'dalam', 'bagi', 'tentang', 'melalui', 'serta', 'maka', 'lagi', 'sudah',\n",
    "        'belum', 'hanya', 'saja', 'bahwa', 'apa', 'siapa', 'bagaimana', 'kapan',\n",
    "        'dimana', 'kenapa', 'sejak', 'hingga', 'agar', 'supaya', 'meskipun', 'walau',\n",
    "        'kecuali', 'terhadap', 'antara', 'selain', 'setiap', 'sebelum', 'sesudah'\n",
    "    ]\n",
    "    try:\n",
    "        vectorizer = TfidfVectorizer(max_features=2000, ngram_range=(1, 2), stop_words=stop_words)\n",
    "        tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "        logger.info(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "        return vectorizer, tfidf_matrix\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up TF-IDF: {e}\")\n",
    "        print(f\"Error setting up TF-IDF: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 6: Extract Features\n",
    "def extract_features(query_vec, doc_vec, query_text: str, doc_text: str) -> np.ndarray:\n",
    "    \"\"\"Extract features for Logistic Regression and SVM.\"\"\"\n",
    "    try:\n",
    "        query_vec = query_vec.toarray()[0]\n",
    "        doc_vec = doc_vec.toarray()[0]\n",
    "        cos_sim = cosine_similarity([query_vec], [doc_vec])[0][0]\n",
    "        query_words = set(query_text.split())\n",
    "        doc_words = set(doc_text.split())\n",
    "        overlap = len(query_words.intersection(doc_words)) / max(len(query_words), 1)\n",
    "        coverage = overlap\n",
    "        return np.concatenate([query_vec, doc_vec, [cos_sim, overlap, coverage]])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting features: {e}\")\n",
    "        return np.zeros(4003)  # 2000 + 2000 + 3\n",
    "\n",
    "# Part 7: Prepare Training Data\n",
    "def prepare_training_data(queries, case_ids, texts, vectorizer, tfidf_matrix):\n",
    "    \"\"\"Prepare training data with balanced sampling.\"\"\"\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for item in queries:\n",
    "        query = preprocess_text(item['query'])\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        true_id = item['case_id']\n",
    "        try:\n",
    "            true_idx = case_ids.index(true_id)\n",
    "        except ValueError:\n",
    "            logger.warning(f\"Case ID {true_id} not found in dataset. Skipping query: {item['query'][:50]}...\")\n",
    "            continue\n",
    "        true_vec = tfidf_matrix[true_idx]\n",
    "        pos_features = extract_features(query_vec, true_vec, query, texts[true_idx])\n",
    "        neg_indices = [i for i in range(len(case_ids)) if i != true_idx]\n",
    "        neg_samples = np.random.choice(neg_indices, size=min(5, len(neg_indices)), replace=False)\n",
    "        for neg_idx in neg_samples:\n",
    "            neg_vec = tfidf_matrix[neg_idx]\n",
    "            neg_features = extract_features(query_vec, neg_vec, query, texts[neg_idx])\n",
    "            X_train.append(pos_features - neg_features)\n",
    "            y_train.append(1)\n",
    "            X_train.append(neg_features - pos_features)\n",
    "            y_train.append(0)\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    logger.info(f\"Training data shape: {X_train.shape}\")\n",
    "    logger.info(f\"Class distribution: {np.bincount(y_train)}\")\n",
    "    return X_train, y_train\n",
    "\n",
    "# Part 8: Train Models\n",
    "def train_models(X_train, y_train):\n",
    "    \"\"\"Train Logistic Regression and SVM with stratified cross-validation.\"\"\"\n",
    "    try:\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        param_grid = {'C': [0.1, 1, 10]}\n",
    "        logreg = GridSearchCV(\n",
    "            LogisticRegression(max_iter=1000, class_weight='balanced', solver='liblinear'),\n",
    "            param_grid, cv=skf, scoring='f1', n_jobs=-1\n",
    "        )\n",
    "        logreg.fit(X_train, y_train)\n",
    "        logger.info(f\"Best Logistic Regression Parameters: {logreg.best_params_}\")\n",
    "        logger.info(f\"Best CV F1 Score (LogReg): {logreg.best_score_:.2f}\")\n",
    "        print(f\"Best Logistic Regression Parameters: {logreg.best_params_}\")\n",
    "        print(f\"Best CV F1 Score (LogReg): {logreg.best_score_:.2f}\")\n",
    "        svm = GridSearchCV(\n",
    "            LinearSVC(max_iter=1000, class_weight='balanced', tol=1e-3),\n",
    "            param_grid, cv=skf, scoring='f1', n_jobs=-1\n",
    "        )\n",
    "        svm.fit(X_train, y_train)\n",
    "        logger.info(f\"Best SVM Parameters: {svm.best_params_}\")\n",
    "        logger.info(f\"Best CV F1 Score (SVM): {svm.best_score_:.2f}\")\n",
    "        print(f\"Best SVM Parameters: {svm.best_params_}\")\n",
    "        print(f\"Best CV F1 Score (SVM): {svm.best_score_:.2f}\")\n",
    "        return logreg, svm\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error training models: {e}\")\n",
    "        print(f\"Error training models: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 9: Setup IndoBERT and BM25\n",
    "def setup_indobert_and_bm25(texts: List[str]):\n",
    "    \"\"\"Initialize IndoBERT and BM25 models.\"\"\"\n",
    "    try:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"Loading IndoBERT model on {device}\")\n",
    "        bi_encoder = SentenceTransformer('indobenchmark/indobert-base-p1', device=device)\n",
    "        cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device, max_length=512)\n",
    "        doc_embeddings = bi_encoder.encode(\n",
    "            texts, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False, batch_size=16\n",
    "        )\n",
    "        bm25 = BM25Okapi([t.split() for t in texts])\n",
    "        logger.info(f\"Document embeddings shape: {doc_embeddings.shape}\")\n",
    "        return bi_encoder, cross_encoder, doc_embeddings, bm25\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error setting up Indo-BERT/BM25: {e}\")\n",
    "        print(f\"Error setting up Indo-BERT/BM25: {e}\")\n",
    "        raise\n",
    "\n",
    "# Part 10: Retrieval Functions\n",
    "def logreg_retrieve(query: str, vectorizer, tfidf_matrix, case_ids, texts, logreg, k: int = 5) -> List[str]:\n",
    "    \"\"\"Retrieve top-k cases using Logistic Regression.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        scores = []\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            doc_vec = tfidf_matrix[i]\n",
    "            features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "            score = logreg.predict_proba([features])[0][1]  # Probability of positive class\n",
    "            scores.append((case_ids[i], score))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [x[0] for x in scores[:k]]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in LogReg retrieval for query '{query[:50]}...': {e}\")\n",
    "        return []\n",
    "\n",
    "def svm_retrieve(query: str, vectorizer, tfidf_matrix, case_ids, texts, svm, k: int = 5) -> List[str]:\n",
    "    \"\"\"Retrieve top-k cases using SVM.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_vec = vectorizer.transform([query])\n",
    "        scores = []\n",
    "        for i in range(tfidf_matrix.shape[0]):\n",
    "            doc_vec = tfidf_matrix[i]\n",
    "            features = extract_features(query_vec, doc_vec, query, texts[i])\n",
    "            score = svm.decision_function([features])[0]\n",
    "            scores.append((case_ids[i], score))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [x[0] for x in scores[:k]]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in SVM retrieval for query '{query[:50]}...': {e}\")\n",
    "        return []\n",
    "\n",
    "def indobert_retrieve(query: str, bi_encoder, cross_encoder, doc_embeddings, bm25, case_ids, texts, k: int = 10, alpha: float = 0.7) -> List[str]:\n",
    "    \"\"\"Retrieve top-k cases using IndoBERT with BM25 hybrid scoring.\"\"\"\n",
    "    try:\n",
    "        query = preprocess_text(query)\n",
    "        query_vec = bi_encoder.encode([query], convert_to_numpy=True, normalize_embeddings=True)[0]\n",
    "        sim_scores = cosine_similarity([query_vec], doc_embeddings)[0]\n",
    "        bm25_scores = bm25.get_scores(query.split())\n",
    "        bm25_scores /= np.max(bm25_scores) + 1e-10\n",
    "        combined = alpha * sim_scores + (1 - alpha) * bm25_scores\n",
    "        top_k_idx = np.argsort(combined)[-k:][::-1]\n",
    "        rerank_pairs = [[query, texts[i]] for i in top_k_idx]\n",
    "        rerank_scores = cross_encoder.predict(rerank_pairs)\n",
    "        reranked_idx = np.argsort(rerank_scores)[::-1][:min(5, len(rerank_scores))]\n",
    "        return [case_ids[top_k_idx[i]] for i in reranked_idx]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in IndoBERT retrieval for query '{query[:50]}...': {e}\")\n",
    "        return []\n",
    "\n",
    "# Part 11: Predict Outcome\n",
    "def predict_outcome(query: str, retrieve_fn, case_solutions, true_solution=None):\n",
    "    \"\"\"Predict outcome based on retrieved cases.\"\"\"\n",
    "    try:\n",
    "        top_5_ids = retrieve_fn(query)\n",
    "        solutions = [case_solutions.get(cid, '') for cid in top_5_ids]\n",
    "        filtered = [s for s in solutions if s and s not in ['nan', None]]\n",
    "        predicted = max(set(filtered), key=filtered.count) if filtered else 'Tidak ditemukan'\n",
    "        metrics = {}\n",
    "        if true_solution and filtered:\n",
    "            y_true = [1 if true_solution == s else 0 for s in solutions]\n",
    "            y_pred = [1 if predicted == s else 0 for s in solutions]\n",
    "            metrics = {\n",
    "                'accuracy': accuracy_score(y_true, y_pred),\n",
    "                'precision': precision_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "                'recall': recall_score(y_true, y_pred, average='binary', zero_division=0),\n",
    "                'f1': f1_score(y_true, y_pred, average='binary', zero_division=0)\n",
    "            }\n",
    "        return predicted, top_5_ids, metrics\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error predicting outcome for query '{query[:50]}...': {e}\")\n",
    "        return 'Tidak ditemukan', [], {}\n",
    "\n",
    "# Part 12: Main Function\n",
    "def main():\n",
    "    \"\"\"Main function to run prediction pipeline.\"\"\"\n",
    "    try:\n",
    "        setup_environment()\n",
    "        df, texts, case_ids, queries, case_solutions = load_data()\n",
    "        vectorizer, tfidf_matrix = setup_tfidf(texts)\n",
    "        X_train, y_train = prepare_training_data(queries, case_ids, texts, vectorizer, tfidf_matrix)\n",
    "        logreg, svm = train_models(X_train, y_train)\n",
    "        bi_encoder, cross_encoder, doc_embeddings, bm25 = setup_indobert_and_bm25(texts)\n",
    "        results_logreg = []\n",
    "        results_svm = []\n",
    "        results_indobert = []\n",
    "        for i in tqdm(range(len(df)), desc='Predicting'):\n",
    "            query_text = df.loc[i, 'ringkasan_fakta']\n",
    "            case_id = df.loc[i, 'case_id']\n",
    "            true_solution = case_solutions.get(case_id, '')\n",
    "            pred_logreg, top_ids_logreg, metrics_logreg = predict_outcome(\n",
    "                query_text,\n",
    "                lambda q: logreg_retrieve(q, vectorizer, tfidf_matrix, case_ids, texts, logreg),\n",
    "                case_solutions,\n",
    "                true_solution\n",
    "            )\n",
    "            results_logreg.append({\n",
    "                'query_id': case_id,\n",
    "                'predicted_solution': pred_logreg,\n",
    "                'top_5_case_ids': ', '.join(top_ids_logreg),\n",
    "                'metrics': metrics_logreg\n",
    "            })\n",
    "            pred_svm, top_ids_svm, metrics_svm = predict_outcome(\n",
    "                query_text,\n",
    "                lambda q: svm_retrieve(q, vectorizer, tfidf_matrix, case_ids, texts, svm),\n",
    "                case_solutions,\n",
    "                true_solution\n",
    "            )\n",
    "            results_svm.append({\n",
    "                'query_id': case_id,\n",
    "                'predicted_solution': pred_svm,\n",
    "                'top_5_case_ids': ', '.join(top_ids_svm),\n",
    "                'metrics': metrics_svm\n",
    "            })\n",
    "            pred_indobert, top_ids_indobert, metrics_indobert = predict_outcome(\n",
    "                query_text,\n",
    "                lambda q: indobert_retrieve(q, bi_encoder, cross_encoder, doc_embeddings, bm25, case_ids, texts),\n",
    "                case_solutions,\n",
    "                true_solution\n",
    "            )\n",
    "            results_indobert.append({\n",
    "                'query_id': case_id,\n",
    "                'predicted_solution': pred_indobert,\n",
    "                'top_5_case_ids': ', '.join(top_ids_indobert),\n",
    "                'metrics': metrics_indobert\n",
    "            })\n",
    "        pd.DataFrame(results_logreg).to_csv(\n",
    "            os.path.join(PATH_RESULTS, 'logreg_predictions.csv'), index=False, encoding='utf-8'\n",
    "        )\n",
    "        pd.DataFrame(results_svm).to_csv(\n",
    "            os.path.join(PATH_RESULTS, 'svm_predictions.csv'), index=False, encoding='utf-8'\n",
    "        )\n",
    "        pd.DataFrame(results_indobert).to_csv(\n",
    "            os.path.join(PATH_RESULTS, 'indobert_predictions.csv'), index=False, encoding='utf-8'\n",
    "        )\n",
    "        logger.info(f\"Saved results to {PATH_RESULTS}/logreg_predictions.csv\")\n",
    "        logger.info(f\"Saved results to {PATH_RESULTS}/svm_predictions.csv\")\n",
    "        logger.info(f\"Saved results to {PATH_RESULTS}/indobert_predictions.csv\")\n",
    "        print(f\"✅ Saved to {PATH_RESULTS}/logreg_predictions.csv\")\n",
    "        print(f\"✅ Saved to {PATH_RESULTS}/svm_predictions.csv\")\n",
    "        print(f\"✅ Saved to {PATH_RESULTS}/indobert_predictions.csv\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in main: {e}\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMs6vGVQUCp3SynA4VIOrs+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
