{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8WElTVlJTdVu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfminer.six in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (20231228)\n",
      "Requirement already satisfied: pandas in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pdfminer.six) (3.4.2)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pdfminer.six) (45.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\faliq\\pictures\\penalaran-komputer\\cbr_penalararan_komputer\\cbr\\notebooks\\venv\\lib\\site-packages (from cffi>=1.14->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pdfminer.six pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WP9Otho3Y6tm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 02:34:39,553 - INFO - Directory ensured: c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\\logs\n",
      "2025-06-26 02:34:39,554 - INFO - Directory ensured: c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\\data\\processed\n",
      "2025-06-26 02:34:39,555 - INFO - Directory ensured: c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\\data\\raw\n",
      "2025-06-26 02:34:39,557 - INFO - Starting metadata extraction process at 2025-06-26 02:34:39\n",
      "2025-06-26 02:34:39,566 - INFO - Processed file: case_001.txt\n",
      "2025-06-26 02:34:39,573 - INFO - Processed file: case_002.txt\n",
      "2025-06-26 02:34:39,579 - INFO - Processed file: case_003.txt\n",
      "2025-06-26 02:34:39,585 - INFO - Processed file: case_004.txt\n",
      "2025-06-26 02:34:39,589 - INFO - Processed file: case_005.txt\n",
      "2025-06-26 02:34:39,594 - INFO - Processed file: case_007.txt\n",
      "2025-06-26 02:34:39,598 - INFO - Processed file: case_008.txt\n",
      "2025-06-26 02:34:39,613 - INFO - Processed file: case_009.txt\n",
      "2025-06-26 02:34:39,630 - INFO - Processed file: case_010.txt\n",
      "2025-06-26 02:34:39,635 - INFO - Processed file: case_011.txt\n",
      "2025-06-26 02:34:39,639 - INFO - Processed file: case_012.txt\n",
      "2025-06-26 02:34:39,648 - INFO - Processed file: case_013.txt\n",
      "2025-06-26 02:34:39,657 - INFO - Processed file: case_014.txt\n",
      "2025-06-26 02:34:39,659 - INFO - Processed file: case_015.txt\n",
      "2025-06-26 02:34:39,661 - INFO - Processed file: case_016.txt\n",
      "2025-06-26 02:34:39,667 - INFO - Processed file: case_017.txt\n",
      "2025-06-26 02:34:39,678 - INFO - Processed file: case_018.txt\n",
      "2025-06-26 02:34:39,682 - INFO - Processed file: case_019.txt\n",
      "2025-06-26 02:34:39,697 - INFO - Processed file: case_020.txt\n",
      "2025-06-26 02:34:39,702 - INFO - Processed file: case_021.txt\n",
      "2025-06-26 02:34:39,705 - INFO - Processed file: case_023.txt\n",
      "2025-06-26 02:34:39,712 - INFO - Processed file: case_024.txt\n",
      "2025-06-26 02:34:39,716 - INFO - Processed file: case_025.txt\n",
      "2025-06-26 02:34:39,721 - INFO - Processed file: case_026.txt\n",
      "2025-06-26 02:34:39,725 - INFO - Processed file: case_027.txt\n",
      "2025-06-26 02:34:39,730 - INFO - Processed file: case_028.txt\n",
      "2025-06-26 02:34:39,734 - INFO - Processed file: case_029.txt\n",
      "2025-06-26 02:34:39,744 - INFO - Processed file: case_030.txt\n",
      "2025-06-26 02:34:39,751 - INFO - Processed file: case_031.txt\n",
      "2025-06-26 02:34:39,754 - INFO - Processed file: case_032.txt\n",
      "2025-06-26 02:34:39,759 - INFO - Processed file: case_033.txt\n",
      "2025-06-26 02:34:39,763 - INFO - Processed file: case_034.txt\n",
      "2025-06-26 02:34:39,767 - INFO - Processed file: case_035.txt\n",
      "2025-06-26 02:34:39,770 - INFO - Processed file: case_036.txt\n",
      "2025-06-26 02:34:39,775 - INFO - Processed file: case_037.txt\n",
      "2025-06-26 02:34:39,778 - INFO - Processed file: case_038.txt\n",
      "2025-06-26 02:34:39,784 - INFO - Processed file: case_039.txt\n",
      "2025-06-26 02:34:39,787 - INFO - Processed file: case_040.txt\n",
      "2025-06-26 02:34:39,804 - INFO - Processed file: case_041.txt\n",
      "2025-06-26 02:34:39,821 - INFO - Processed file: case_042.txt\n",
      "2025-06-26 02:34:39,825 - INFO - Processed file: case_043.txt\n",
      "2025-06-26 02:34:39,830 - INFO - Processed file: case_044.txt\n",
      "2025-06-26 02:34:39,833 - INFO - Processed file: case_045.txt\n",
      "2025-06-26 02:34:39,837 - INFO - Processed file: case_046.txt\n",
      "2025-06-26 02:34:39,844 - INFO - Processed file: case_047.txt\n",
      "2025-06-26 02:34:39,847 - INFO - Processed file: case_048.txt\n",
      "2025-06-26 02:34:39,854 - INFO - Processed file: case_049.txt\n",
      "2025-06-26 02:34:39,858 - INFO - Processed file: case_050.txt\n",
      "2025-06-26 02:34:39,961 - INFO - Saved 48 metadata entries to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\\data\\processed\\cases.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 48 files and saved to c:\\Users\\Faliq\\Pictures\\PENALARAN-KOMPUTER\\CBR_Penalararan_Komputer\\CBR\\data\\processed\\cases.csv\n"
     ]
    }
   ],
   "source": [
    "# Part 1: Import Libraries and Initialize Directories\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Define base directory (aligned with 01_scraper.py)\n",
    "try:\n",
    "    BASE_DIR = os.path.dirname(os.getcwd())  # Parent of 'notebooks'\n",
    "except NameError:\n",
    "    BASE_DIR = os.getcwd()  # Fallback for interactive environments like Jupyter\n",
    "\n",
    "# Define paths\n",
    "PATH_OUTPUT = os.path.join(BASE_DIR, 'data', 'raw')\n",
    "PATH_CSV = os.path.join(BASE_DIR, 'data', 'processed', 'cases.csv')\n",
    "LOG_DIR = os.path.join(BASE_DIR, 'logs')\n",
    "LOG_PATH = os.path.join(LOG_DIR, 'metadata_extraction.log')\n",
    "\n",
    "# Validate path length for Windows\n",
    "MAX_PATH_LENGTH = 260\n",
    "\n",
    "def validate_path(path):\n",
    "    if len(path) > MAX_PATH_LENGTH:\n",
    "        raise ValueError(f\"Path {path} exceeds Windows maximum length of {MAX_PATH_LENGTH} characters\")\n",
    "    return path\n",
    "\n",
    "# Ensure directories exist\n",
    "for path in [LOG_DIR, os.path.dirname(PATH_CSV), PATH_OUTPUT]:\n",
    "    try:\n",
    "        validate_path(path)\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        logging.info(f\"Directory ensured: {path}\")\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Path validation failed: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create directory {path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_PATH, mode='w', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ],\n",
    "    force=True\n",
    ")\n",
    "logging.info(\"Starting metadata extraction process at %s\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "# Part 2: Utility Functions\n",
    "def clean_name(name):\n",
    "    titles = [r'dr\\.', r'drh\\.', r'sh\\.', r'mh\\.', r'm\\.h\\.', r's\\.h\\.', r'prof\\.', r'ir\\.', r'hj\\.', r'h\\.']\n",
    "    for title in titles:\n",
    "        name = re.sub(title, '', name, flags=re.IGNORECASE)\n",
    "    return ' '.join(name.split()).strip()\n",
    "\n",
    "month_map = {\n",
    "    'januari': 'January', 'februari': 'February', 'maret': 'March',\n",
    "    'april': 'April', 'mei': 'May', 'juni': 'June', 'juli': 'July',\n",
    "    'agustus': 'August', 'september': 'September', 'oktober': 'October',\n",
    "    'november': 'November', 'desember': 'December'\n",
    "}\n",
    "\n",
    "# Part 3: Metadata Extraction Function\n",
    "def extract_metadata(text, file_name):\n",
    "    metadata = {\n",
    "        'case_id': file_name.replace('.txt', ''),\n",
    "        'nomor_perkara': '', 'tahun_putusan': '', 'bulan_putusan': '',\n",
    "        'tanggal_putusan': '', 'jenis_perkara': '', 'tingkat_pemeriksaan': '',\n",
    "        'lembaga_peradilan': '', 'pasal': '', 'hakim_ketua': '',\n",
    "        'ringkasan_fakta': '', 'jumlah_kata_putusan': 0, 'full_text': text\n",
    "    }\n",
    "\n",
    "    text = ' '.join(text.split())  # Normalize spacing\n",
    "\n",
    "    # Nomor perkara\n",
    "    patterns = [\n",
    "        r'(?:putusan|penetapan)\\s*nomor\\s*([\\w/\\s]+?\\d{4})',\n",
    "        r'nomor\\s*([\\w/\\s]+?\\d{4})'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text, re.IGNORECASE)\n",
    "        if match:\n",
    "            metadata['nomor_perkara'] = re.sub(r'\\s+', ' ', match.group(1)).strip()\n",
    "            break\n",
    "    if not metadata['nomor_perkara']:\n",
    "        logging.warning(f\"No nomor_perkara found in {file_name}. Sample: {text[:200]}\")\n",
    "\n",
    "    # Case year\n",
    "    match = re.search(r'(\\d{4})$', metadata['nomor_perkara'])\n",
    "    if match:\n",
    "        metadata['tahun_putusan'] = match.group(1)\n",
    "\n",
    "    # Tanggal putusan\n",
    "    date_patterns = [\n",
    "        r'(\\d{1,2}\\s+\\w+\\s+\\d{4})'\n",
    "    ]\n",
    "    for pattern in date_patterns:\n",
    "        for m in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            date_text = m.group(1)\n",
    "            for id_m, en_m in month_map.items():\n",
    "                date_text = re.sub(rf'\\b{id_m}\\b', en_m, date_text, flags=re.IGNORECASE)\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_text, '%d %B %Y')\n",
    "                metadata['tanggal_putusan'] = str(date_obj.day)\n",
    "                metadata['bulan_putusan'] = date_obj.strftime('%B')\n",
    "                metadata['tahun_putusan'] = metadata['tahun_putusan'] or str(date_obj.year)\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Jenis perkara\n",
    "    jp_map = {'pidana': 'Pidana', 'perdata': 'Perdata', 'anak': 'Pidana Anak', 'narkotika': 'Pidana Narkotika'}\n",
    "    for key, val in jp_map.items():\n",
    "        if key in text.lower():\n",
    "            metadata['jenis_perkara'] = val\n",
    "            break\n",
    "\n",
    "    # Tingkat pemeriksaan\n",
    "    if 'peninjauan kembali' in text.lower():\n",
    "        metadata['tingkat_pemeriksaan'] = 'Peninjauan Kembali'\n",
    "    elif 'kasasi' in text.lower():\n",
    "        metadata['tingkat_pemeriksaan'] = 'Kasasi'\n",
    "    elif 'banding' in text.lower():\n",
    "        metadata['tingkat_pemeriksaan'] = 'Banding'\n",
    "    else:\n",
    "        metadata['tingkat_pemeriksaan'] = 'Pertama'\n",
    "\n",
    "    # Lembaga peradilan\n",
    "    lembaga_map = [\n",
    "        ('mahkamah agung', 'Mahkamah Agung'),\n",
    "        ('pengadilan tinggi', 'Pengadilan Tinggi'),\n",
    "        ('pengadilan agama', 'Pengadilan Agama'),\n",
    "        ('pengadilan negeri', 'Pengadilan Negeri')\n",
    "    ]\n",
    "    for k, v in lembaga_map:\n",
    "        if k in text.lower():\n",
    "            metadata['lembaga_peradilan'] = v\n",
    "            break\n",
    "    else:\n",
    "        metadata['lembaga_peradilan'] = 'Unknown'\n",
    "\n",
    "    # Pasal\n",
    "    match = re.search(r'(pasal\\s+\\d+(?:\\s+ayat\\s+\\d+)?)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        metadata['pasal'] = match.group(1)\n",
    "\n",
    "    # Hakim Ketua\n",
    "    match = re.search(r'(?:ketua\\s+majelis|hakim\\s+ketua)[:\\s]*([^\\n,]+)', text, re.IGNORECASE)\n",
    "    if match:\n",
    "        metadata['hakim_ketua'] = clean_name(match.group(1))\n",
    "\n",
    "    # Ringkasan fakta\n",
    "    patterns = [r'(?:terdakwa|terpidana).*?dakwaan.*?berikut\\s+(.+?)\\s+(?:menimbang|demikian)', r'mengadili\\s+(.+?)\\s+(?:menimbang|demikian)']\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, text, re.IGNORECASE | re.DOTALL)\n",
    "        if m:\n",
    "            summary = ' '.join(m.group(1).split())\n",
    "            metadata['ringkasan_fakta'] = summary[:1000] + '...' if len(summary) > 1000 else summary\n",
    "            break\n",
    "\n",
    "    metadata['jumlah_kata_putusan'] = len(text.split())\n",
    "    return metadata\n",
    "\n",
    "# Part 4: CSV Saving Function\n",
    "def save_to_csv(metadata_list):\n",
    "    fieldnames = [\n",
    "        'case_id', 'nomor_perkara', 'tahun_putusan', 'bulan_putusan', 'tanggal_putusan',\n",
    "        'jenis_perkara', 'tingkat_pemeriksaan', 'lembaga_peradilan', 'pasal',\n",
    "        'hakim_ketua', 'ringkasan_fakta', 'jumlah_kata_putusan', 'full_text'\n",
    "    ]\n",
    "    with open(PATH_CSV, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in metadata_list:\n",
    "            writer.writerow(row)\n",
    "        logging.info(f\"Saved {len(metadata_list)} metadata entries to {PATH_CSV}\")\n",
    "\n",
    "# Part 5: Main Processing\n",
    "\n",
    "def process_text_files():\n",
    "    if not os.path.exists(PATH_OUTPUT):\n",
    "        logging.error(f\"Directory {PATH_OUTPUT} does not exist.\")\n",
    "        return\n",
    "\n",
    "    files = [f for f in os.listdir(PATH_OUTPUT) if f.endswith('.txt')]\n",
    "    if not files:\n",
    "        logging.warning(f\"No .txt files found in {PATH_OUTPUT}\")\n",
    "        return\n",
    "\n",
    "    metadata_list = []\n",
    "    for file in files:\n",
    "        path = os.path.join(PATH_OUTPUT, file)\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read().strip()\n",
    "        except UnicodeDecodeError:\n",
    "            with open(path, 'r', encoding='latin-1') as f:\n",
    "                text = f.read().strip()\n",
    "        if text:\n",
    "            metadata = extract_metadata(text, file)\n",
    "            metadata_list.append(metadata)\n",
    "            logging.info(f\"Processed file: {file}\")\n",
    "\n",
    "    if metadata_list:\n",
    "        save_to_csv(metadata_list)\n",
    "        print(f\"Processed {len(metadata_list)} files and saved to {PATH_CSV}\")\n",
    "    else:\n",
    "        logging.warning(\"No metadata extracted\")\n",
    "\n",
    "# Part 6: Entry Point\n",
    "if __name__ == \"__main__\":\n",
    "    process_text_files()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNOSfY7VVwfgV/KbZAz+5/E",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
